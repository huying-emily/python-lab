{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data processsing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(7)\n",
    "\n",
    "import nltk\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def remove_stopwords(X):\n",
    "    word_tokens = word_tokenize(X)\n",
    "    stop_words = pd.read_csv(\"data/stop_words.csv\")[\"stop word\"].tolist()\n",
    "    X = [w for w in word_tokens if not w in stop_words]\n",
    "    X = ' '.join(X)\n",
    "    return X\n",
    "        \n",
    "def stemming(X):\n",
    "    stemmer = PorterStemmer()\n",
    "    word_tokens = word_tokenize(X)\n",
    "    X = [stemmer.stem(w) for w in word_tokens]\n",
    "    X = ' '.join(X)\n",
    "    return X\n",
    "\n",
    "def remove_noun(X):\n",
    "    X = nltk.tag.pos_tag(X.split())\n",
    "    X = [word for word, tag in X if tag != 'NNP' and tag != 'NNPS' and tag != 'NN']\n",
    "    X = ' '.join(X)\n",
    "    return X\n",
    "\n",
    "def cleaning(X):    \n",
    "    X = [re.sub(r'[^\\x00-\\x7F]+','', twit) for twit in X]\n",
    "    X = [re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",'', twit) for twit in X]\n",
    "    X = [re.sub(r'http\\S+', '', twit) for twit in X]\n",
    "    X = [re.sub(r'#', '', twit) for twit in X]\n",
    "    X = [re.sub(r'@', '', twit) for twit in X]\n",
    "    X = [twit.lower() for twit in X]\n",
    "    X = [remove_stopwords(twit) for twit in X]\n",
    "    return X\n",
    "\n",
    "def two_grams(twit):\n",
    "    n = 2\n",
    "    tokens = ngrams(twit.split(), n)\n",
    "    X = [token for token in tokens]\n",
    "    return X\n",
    "def unit_grams(twit):\n",
    "    n = 1\n",
    "    tokens = ngrams(twit.split(), n)\n",
    "    X = [token for token in tokens]\n",
    "    return X\n",
    "def ngrams_features(X):\n",
    "    X_1 = [unit_grams(twit) for twit in X] \n",
    "    X_2 = [two_grams(twit) for twit in X]\n",
    "    return X_1, X_2\n",
    "\n",
    "def find_dict(X, top_words):\n",
    "    ### use NLTK ######\n",
    "    # Tokenize\n",
    "    # every element in X will be transfered to a feature vector, \n",
    "    # each element in the feature vector represents the occrence of a word\n",
    "    # the dimenson of the vector equals to the top_word.\n",
    "    all_words = []\n",
    "    for words in X:\n",
    "        all_words = all_words + words\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    print(\"the size of the vocab is: \", len(all_words))\n",
    "    word_features = list(all_words.keys())[:top_words]\n",
    "    dict_words = {}\n",
    "    for count, word in enumerate(word_features):\n",
    "        dict_words[word] = count\n",
    "    return dict_words\n",
    "\n",
    "def find_features(twit, dict_words):\n",
    "    features = []\n",
    "    for word in twit:\n",
    "        if word in dict_words:\n",
    "            features.append(dict_words[word])\n",
    "    return features\n",
    "\n",
    "def text2features(X_raw):\n",
    "    X_text = cleaning(X_raw)\n",
    "    X_1, X_2 = ngrams_features(X_text)\n",
    "    X_train = X_1\n",
    "    dict_words = find_dict(X_train, vocab_size)\n",
    "    print(\"the dictionary is\", take(10, dict_words.items()))\n",
    "    X_train = [find_features(twit, dict_words) for twit in X_train] \n",
    "    i = 10\n",
    "    print(f\"the {i+1}th sample is: \", X_train[i])\n",
    "    return X_train, dict_words\n",
    "\n",
    "def discrete_sentiment(y):\n",
    "    y_d = []\n",
    "    for label in y:\n",
    "        if label < -0.20:\n",
    "            y_d.append(0)\n",
    "        elif label < 0.25:\n",
    "            y_d.append(1)\n",
    "        else:\n",
    "            y_d.append(2)\n",
    "    return y_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the vocab is:  2647\n",
      "the dictionary is [(('book',), 0), (('second',), 1), (('consecutive',), 2), (('quarter',), 3), (('sales',), 4), (('growth',), 5), (('posts',), 6), (('drop',), 7), (('first-quarter',), 8), (('organic',), 9)]\n",
      "the 11th sample is:  [19, 62, 30, 63, 64, 65, 66, 67, 68]\n",
      "training labels: Counter({2: 401, 1: 399, 0: 342})\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "##############################################\n",
    "#####    prepare the data ##########\n",
    "##############################################\n",
    "##############################################\n",
    "\n",
    "import json\n",
    "with open('data/Headline_Trainingdata.json') as f:\n",
    "    data = json.load(f)\n",
    "vocab_size = 2500\n",
    "df_raw = pd.DataFrame(data=data)\n",
    "df_raw.sample(frac=1)\n",
    "company_name = df_raw[\"company\"].tolist()\n",
    "X_raw = df_raw[\"title\"].tolist()\n",
    "y_raw = df_raw[\"sentiment\"].tolist()\n",
    "X_raw = [twit.replace(company_name[i],  '') for i, twit in enumerate(X_raw)]\n",
    "X_train = list(text2features(X_raw)[0])\n",
    "y_train = discrete_sentiment(y_raw)\n",
    "from collections import Counter\n",
    "print(\"training labels:\", Counter(y_train))\n",
    "\n",
    "y_encoded = np.zeros((len(y_train), 3),  dtype=int)\n",
    "y_encoded[np.asarray(y_train) == 0, 0] = 1\n",
    "y_encoded[np.asarray(y_train) == 1, 1] = 1\n",
    "y_encoded[np.asarray(y_train) == 2, 2] = 1\n",
    "\n",
    "max_review_length = 14\n",
    "num_test = 150\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "\n",
    "X_test = X_train[:num_test]\n",
    "X = X_train[num_test:]\n",
    "y_test = y_encoded[:num_test]\n",
    "y = y_encoded[num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 14, 100)      250000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 14, 50)       5050        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 13, 20)       4020        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 12, 15)       4515        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 50)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 20)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 15)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 85)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           5504        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            195         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 3)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 269,284\n",
      "Trainable params: 269,284\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import MaxPooling1D, Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "tweet_input = Input(shape=(max_review_length,), dtype='int32')\n",
    "tweet_encoder = Embedding(vocab_size, 100, input_length=max_review_length)(tweet_input)\n",
    "\n",
    "unigram_branch = Conv1D(filters=50, kernel_size=1, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "unigram_branch = GlobalMaxPooling1D()(unigram_branch)\n",
    "# unigram_branch = Dropout(0.9)(unigram_branch)\n",
    "bigram_branch = Conv1D(filters=20, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "# bigram_branch = Dropout(0.9)(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=15, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "# trigram_branch = Dropout(0.9)(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=10, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "# fourgram_branch = Dropout(0.9)(fourgram_branch)\n",
    "fivegram_branch = Conv1D(filters=5, kernel_size=5, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fivegram_branch = GlobalMaxPooling1D()(fivegram_branch)\n",
    "# fivegram_branch = Dropout(0.9)(fivegram_branch)\n",
    "\n",
    "merged = concatenate([\n",
    "    unigram_branch, \n",
    "    bigram_branch, \n",
    "    trigram_branch, \n",
    "#    fourgram_branch, \n",
    "#    fivegram_branch, \n",
    "#    sixgram_branch\n",
    "], axis=1)\n",
    "\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = Dense(3)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 892 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "892/892 [==============================] - 0s 120us/step - loss: 0.0230 - accuracy: 0.9944 - val_loss: 1.7907 - val_accuracy: 0.5900\n",
      "Epoch 2/10\n",
      "892/892 [==============================] - 0s 119us/step - loss: 0.0181 - accuracy: 0.9955 - val_loss: 1.8694 - val_accuracy: 0.5500\n",
      "Epoch 3/10\n",
      "892/892 [==============================] - 0s 120us/step - loss: 0.0201 - accuracy: 0.9944 - val_loss: 1.9797 - val_accuracy: 0.5500\n",
      "Epoch 4/10\n",
      "892/892 [==============================] - 0s 116us/step - loss: 0.0233 - accuracy: 0.9955 - val_loss: 2.0100 - val_accuracy: 0.5500\n",
      "Epoch 5/10\n",
      "892/892 [==============================] - 0s 115us/step - loss: 0.0254 - accuracy: 0.9944 - val_loss: 1.9957 - val_accuracy: 0.5500\n",
      "Epoch 6/10\n",
      "892/892 [==============================] - 0s 121us/step - loss: 0.0095 - accuracy: 0.9989 - val_loss: 2.0583 - val_accuracy: 0.5700\n",
      "Epoch 7/10\n",
      "892/892 [==============================] - 0s 120us/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 2.1128 - val_accuracy: 0.5600\n",
      "Epoch 8/10\n",
      "892/892 [==============================] - 0s 121us/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 2.1928 - val_accuracy: 0.5500\n",
      "Epoch 9/10\n",
      "892/892 [==============================] - 0s 120us/step - loss: 0.0135 - accuracy: 0.9955 - val_loss: 2.2260 - val_accuracy: 0.5700\n",
      "Epoch 10/10\n",
      "892/892 [==============================] - 0s 120us/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 2.2187 - val_accuracy: 0.5500\n",
      "test labels: Counter({1: 57, 0: 48, 2: 45})\n",
      "the accuracy for the testing data set is:  0.64\n",
      "[[29  8 11]\n",
      " [12 38  7]\n",
      " [ 3 13 29]]\n"
     ]
    }
   ],
   "source": [
    "num_train = len(X_raw) - num_test\n",
    "training_size = [\n",
    "#        200, \n",
    "#        400, \n",
    "#        600, \n",
    "#        800, \n",
    "        num_train]\n",
    "acc_list = []\n",
    "for size in training_size:\n",
    "    model.fit(X[:size], y[:size], batch_size=32, epochs=10, validation_split=0.1)\n",
    "    y_predic = model.predict(X_test)\n",
    "    predic = np.argmax(y_predic, 1)\n",
    "    label = np.argmax(y_test, 1)\n",
    "\n",
    "    results = (predic == label)\n",
    "    accuracy = sum(results)/num_test\n",
    "    print(\"test labels:\", Counter(label))\n",
    "    #print(\"train labels:\", Counter(np.argmax(y, 1)))\n",
    "    #print(\"all labels:\", Counter(np.argmax(y_encoded, 1)))\n",
    "    df_results = pd.DataFrame(data={\"news\":X_raw[:num_test], \"label\":label, \"prediction\":predic, \"results\":results})\n",
    "    df_results.to_csv(\"results/sentiment_results_1031.csv\")\n",
    "    print(\"the accuracy for the testing data set is: \", accuracy)\n",
    "    acc_list.append(accuracy)\n",
    "    cm = confusion_matrix(label, predic)\n",
    "    print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
