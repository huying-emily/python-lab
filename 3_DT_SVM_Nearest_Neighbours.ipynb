{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "3 DT SVM Nearest Neighbours.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtvjfRCc_JXk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The **Australian** credit approval dataset illustrates multiple practical issues.\n",
        "* Missing feature values: This is very common in practice. This can be handled in various ways.\n",
        " * Removing instances with missing feature values. This may be fine if the number of such instances is small. However, the issue still has to be handled when the predictor is deployed.\n",
        " * Imputing (predicting and filling in) the missing values. May affect performance if prediction is poor.\n",
        " * Encoding the missing value as a special value. This may be appropriate if the value is not missing at random and being missing actually provides some information.\n",
        " * Algorithm specific method. Decision trees have specific methods for handling missing values (e.g. averaging over both paths down the tree at the missing node) but this is not implemented in Scikit Learn. Generative models handle missing values naturally as part of probabilistic inference.\n",
        "* The Australian dataset has a mix of continuous and categorical feature.\n",
        "* For confidentiality purposes, feature names and values have been changed into meaningless symbols in the dataset. As a result, we cannot use our exploit knowledge of the problem to construct better predictors. One question is how to handle the categorical feature.\n",
        " * If the feature is an ordinal variable, i.e. the values are ordered, it may sometimes be useful to map the values to integers or reals, particularly if we expect the target value to change monotonically with the value of the feature.\n",
        " * If the feature is a nominal variable, i.e. the values cannot be ordered, mapping the values to integers or reals may not make sense. Some decision tree algorithms can do a multiway split at the variable with one child for each possible variable value. However, Scikit Learn simply treats all variables as integers/reals and do binary tests with $\\leq$.\n",
        " \n",
        " \n",
        "In this dataset, missing values are handled by imputation: mean values are used for continuous variables, and mode is used for categorical variables. For datasets like this, one strength of decision trees is that it is somewhat less sensitive to appropriate processing of variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9ayOQIhPdJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2f737d47-a67b-4d45-d44c-6d58c4d0dc42"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VERAdGWk_JXl",
        "colab_type": "code",
        "outputId": "2693bfd7-52ea-46ea-862b-98c65c4d8637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import neighbors\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os as os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/session 2\")\n",
        "\n",
        "X, y = load_svmlight_file(\"australian.txt\")\n",
        "# Should really repeat this with many random splits to get more reliable results; try various splits\n",
        "# by changing random_state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "# Scale and split data\n",
        "scaler = preprocessing.MaxAbsScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Decision tree on original data\n",
        "clf = tree.DecisionTreeClassifier(random_state=42)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "predict = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "print(\"DT accuracy with original data: \" + \"{0:.2f}\".format(accuracy))\n",
        "\n",
        "# Decision tree on scaled data\n",
        "clf = tree.DecisionTreeClassifier(random_state=42)\n",
        "clf = clf.fit(X_train_scaled, y_train)\n",
        "predict = clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "print(\"DT accuracy with scaled data: \" + \"{0:.2f}\".format(accuracy) + \"\\n\")\n",
        "\n",
        "# Nearest neighbour on original data\n",
        "clf = neighbors.KNeighborsClassifier(1)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "predict = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "print(\"NN accuracy with original data: \" + \"{0:.2f}\".format(accuracy))\n",
        "\n",
        "# Nearest neighbour on scaled data\n",
        "clf = neighbors.KNeighborsClassifier(1)\n",
        "clf = clf.fit(X_train_scaled, y_train)\n",
        "predict = clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "print(\"NN accuracy with scaled data: \" + \"{0:.2f}\".format(accuracy) + \"\\n\")\n",
        "\n",
        "# Boosted decision tree on original data\n",
        "clf = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=5),\n",
        "                         n_estimators=50,random_state=42)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "predict = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "print(\"Boosted decision tree accuracy with original data: \" + \"{0:.2f}\".format(accuracy))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DT accuracy with original data: 0.82\n",
            "DT accuracy with scaled data: 0.81\n",
            "\n",
            "NN accuracy with original data: 0.67\n",
            "NN accuracy with scaled data: 0.80\n",
            "\n",
            "Boosted decision tree accuracy with original data: 0.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB8mpTzQ_JXq",
        "colab_type": "text"
      },
      "source": [
        "In the **Adult Census dataset** we are given information about individuals obtained from the US census and asked to predict whether the person earns more than US\\$50K a year. The features consist of \n",
        "* Continuous features \"Age\", \"fnlwgt\", \"Education-Num\", \"Capital Gain\", \"Capital Loss\", \"Hours per week\", and\n",
        "* Categorical features \"Workclass\", \"Education\", \"Marital Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Country\". \n",
        "\n",
        "Categorical features cannot be used directly in a linear classifier. One simple way to handle this is to simply map each category of a feature to an integer.\n",
        "\n",
        "**Archipelago:** Before running the exercise, think of a potentially better way to handle categorical variable and write it down. Run the experiment, then discuss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbtxCr5D_JXr",
        "colab_type": "code",
        "outputId": "071aad4c-1370-4825-a0b0-169d2aa2dfbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import os as os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/session 2\")\n",
        "\n",
        "feature_names = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\n",
        "                 \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
        "                 \"Hours per week\", \"Country\"]\n",
        "data = np.genfromtxt('adult.txt', delimiter=', ', dtype=str)\n",
        "\n",
        "# Extract labels\n",
        "labels = data[:,14]\n",
        "le= preprocessing.LabelEncoder()\n",
        "le.fit(labels)\n",
        "labels = le.transform(labels)\n",
        "class_names = le.classes_\n",
        "data = data[:,:-1]\n",
        "\n",
        "# Transform categorical features into integers\n",
        "categorical_features = [1,3,5,6,7,8,9,13]\n",
        "categorical_names = {}\n",
        "for feature in categorical_features:\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(data[:, feature])\n",
        "    data[:, feature] = le.transform(data[:, feature])\n",
        "    categorical_names[feature] = le.classes_\n",
        "data = data.astype(float)\n",
        "\n",
        "# Scale and split data\n",
        "scaler = preprocessing.MaxAbsScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_scaled, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# SVM classifier\n",
        "clf = svm.LinearSVC(loss='hinge', C=1, random_state = 42)\n",
        "clf.fit(X_train, y_train)\n",
        "predict = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "print(\"SVM accuracy: \" + \"{0:.2f}\".format(accuracy))\n",
        "\n",
        "# Now use one hot encoder for categorical features\n",
        "encoder = preprocessing.OneHotEncoder(categories = \"auto\")\n",
        "encoder.fit(data)\n",
        "print(data.shape)\n",
        "\n",
        "encoded_data = encoder.transform(data)\n",
        "data_onehot_scaled = scaler.fit_transform(encoded_data)\n",
        "print(data_onehot_scaled.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_onehot_scaled, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# SVM on data with categorical features encoded using one hot encoding\n",
        "clf = svm.LinearSVC(loss='hinge', C=1, random_state = 42)\n",
        "clf.fit(X_train, y_train)\n",
        "predict = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "print(\"SVM one hot encoding accuracy: \" + \"{0:.2f}\".format(accuracy))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM accuracy: 0.82\n",
            "(32561, 14)\n",
            "(32561, 22144)\n",
            "SVM one hot encoding accuracy: 0.86\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOGmCGiR_JXv",
        "colab_type": "text"
      },
      "source": [
        "Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rASCwXNT_JXw",
        "colab_type": "code",
        "outputId": "7dadc373-06da-40e0-c9c3-a5ed22ad074a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn import neighbors\n",
        "from sklearn import tree\n",
        "\n",
        "# Select only 4 categories to speed things up\n",
        "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
        "\n",
        "# Fetch training and test sets\n",
        "twenty_train = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'),\n",
        "                                  categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test', remove=('headers','footers','quotes'),\n",
        "                                 categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "# Use tfidf\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(twenty_train.data)\n",
        "vectors_test = vectorizer.transform(twenty_test.data)\n",
        "\n",
        "clf = svm.LinearSVC(loss='hinge', C=100)\n",
        "clf.fit(vectors, twenty_train.target)\n",
        "predict = clf.predict(vectors_test)\n",
        "accuracy = accuracy_score(twenty_test.target, predict)\n",
        "print(\"SVM accuracy: \" + \"{0:.2f}\".format(accuracy))\n",
        "\n",
        "fs = SelectKBest(chi2, k=100)\n",
        "vectors_fs = fs.fit_transform(vectors, twenty_train.target)\n",
        "vectors_test_fs = fs.transform(vectors_test)\n",
        "\n",
        "clf = neighbors.KNeighborsClassifier(1)\n",
        "clf.fit(vectors_fs, twenty_train.target)\n",
        "predict = clf.predict(vectors_test_fs)\n",
        "accuracy = accuracy_score(twenty_test.target, predict)\n",
        "print(\"NN accuracy with 100 features (prev best NN performer): \" + \"{0:.2f}\".format(accuracy))\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(vectors, twenty_train.target)\n",
        "predict = clf.predict(vectors_test)\n",
        "accuracy = accuracy_score(twenty_test.target, predict)\n",
        "print(\"Decision Tree accuracy of : \" + \"{0:.2f}\".format(accuracy))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM accuracy: 0.82\n",
            "NN accuracy with 100 features (prev best NN performer): 0.60\n",
            "Decision Tree accuracy of : 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gl3DoUG_JXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}