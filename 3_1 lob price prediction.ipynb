{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "orderbook = \"GOOG_2012-06-21_34200000_57600000_orderbook_10.csv\"\n",
    "message = \"GOOG_2012-06-21_34200000_57600000_message_10.csv\"\n",
    "num_levels = 10\n",
    "\n",
    "header_list = []\n",
    "for i in range(num_levels):\n",
    "    header_list = header_list + [\"Pa%d\"%(i+1),\"Va%d\"%(i+1),\"Pb%d\"%(i+1),\"Vb%d\"%(i+1)]\n",
    "df_orderbook = pd.read_csv(orderbook,header=None,names=header_list)\n",
    "\n",
    "df_message = pd.read_csv(message,usecols = [0,1,3,4,5], names=['time', 'type','size','price','direction'])\n",
    "df_message.index = pd.Timestamp(datetime.date.today()) + pd.TimedeltaIndex(df_message.time, unit='s')\n",
    "df_orderbook.index = df_message.index\n",
    "\n",
    "def labelling(a):\n",
    "    if a > 0:\n",
    "        b = 1\n",
    "    else:\n",
    "        b= 0\n",
    "    return b\n",
    "\n",
    "# Spreads and mid-prices\n",
    "def feature_v2(num_levels,df): # 20\n",
    "    for i in range(1,num_levels+1):\n",
    "        df[\"spread%d\"%(i)] = df[\"Pa%d\"%(i)] - df[\"Pb%d\"%(i)]\n",
    "        df[\"midprice%d\"%(i)] = (df[\"Pa%d\"%(i)] + df[\"Pb%d\"%(i)])/2\n",
    "    return df\n",
    "\n",
    "# Price differences on bid and ask separately\n",
    "def feature_v3(num_levels,df): # 20 - 2\n",
    "    for i in range(1, num_levels):\n",
    "        df[\"PA_diff%d\"%(i)] = df[\"Pa%d\"%(i+1)] - df[\"Pa%d\"%(i)]\n",
    "        df[\"PB_diff%d\"%(i)] = df[\"Pb%d\"%(i)] - df[\"Pb%d\"%(i+1)]\n",
    "    return df\n",
    "\n",
    "# average price and volumes\n",
    "def feature_v4(num_levels,df): # 4\n",
    "    lst = [\"Pa%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Pa_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)    \n",
    "    \n",
    "    lst = [\"Pb%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Pb_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)\n",
    "    \n",
    "    lst = [\"Va%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Va_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)\n",
    "    \n",
    "    lst = [\"Vb%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Vb_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "# Accumulative differences between bid and ask for each level \n",
    "def feature_v5(num_levels,df): # 20\n",
    "    for i in range(num_levels): #\n",
    "        df[\"pri_accum_diff%d\"%(i+1)] = 0\n",
    "        df[\"vol_accum_diff%d\"%(i+1)] = 0\n",
    "        for k in range(i):\n",
    "            df[\"pri_accum_diff%d\"%(i+1)] += (df[\"Pa%d\"%(k+1)] - df[\"Pb%d\"%(k+1)])\n",
    "            df[\"vol_accum_diff%d\"%(i+1)] += (df[\"Va%d\"%(i+1)] - df[\"Vb%d\"%(i+1)])\n",
    "    return df\n",
    "\n",
    "def normalize_input(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only.\n",
    "    scaler.fit(X_train)\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "df_orderbook = feature_v2(num_levels,df_orderbook)\n",
    "df_orderbook = feature_v3(num_levels,df_orderbook)\n",
    "df_orderbook = feature_v4(num_levels,df_orderbook)\n",
    "df_orderbook = feature_v5(num_levels,df_orderbook)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "def model_scoring(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"======================================\")\n",
    "    print(\"======Model Performance===============\")\n",
    "\n",
    "    print(\"the accuracy is: \", accuracy)\n",
    "    print(\"the precision is：\",precision)\n",
    "    print(\"the recall is: \", recall)\n",
    "    print(\"the f1 score is: \", f1)\n",
    "    print(\"confution matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "def split_sequence(X_sequence, y_sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(0, len(X_sequence)-n_steps+1):\n",
    "        seq_x, seq_y = X_sequence[i: i+n_steps], y_sequence[i + n_steps - 1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "df_orderbook1s = df_orderbook.resample('1S').first()\n",
    "df_orderbook1s[\"price_10min\"] = df_orderbook1s['midprice1'].shift(-600)\n",
    "df_orderbook1s.dropna(inplace=True)\n",
    "df_orderbook1s[\"price_change\"] = df_orderbook1s[\"price_10min\"] - df_orderbook1s['midprice1']\n",
    "\n",
    "X = df_orderbook1s.drop(['price_10min', 'price_change'], axis=1).values\n",
    "y = np.array(list(map(labelling, df_orderbook1s[\"price_change\"])))\n",
    "####################################################################\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743, 102)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743, 102)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score. 0.686968085106383\n",
      "testing score. 0.6928327645051194\n",
      "======================================\n",
      "======Model Performance===============\n",
      "the accuracy is:  0.6928327645051194\n",
      "the precision is： 0.5833333333333334\n",
      "the recall is:  0.10294117647058823\n",
      "the f1 score is:  0.17500000000000002\n",
      "confution matrix: \n",
      " [[2128   75]\n",
      " [ 915  105]]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"training score.\", clf.score(X_train, y_train))\n",
    "print(\"testing score.\", clf.score(X_test, y_test))\n",
    "y_pred = clf.predict(X_test)\n",
    "model_scoring(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "======Model Performance===============\n",
      "the accuracy is:  0.8245695672405771\n",
      "the precision is： 0.8057692307692308\n",
      "the recall is:  0.6028776978417266\n",
      "the f1 score is:  0.6897119341563785\n",
      "confution matrix: \n",
      " [[1353  101]\n",
      " [ 276  419]]\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    " ######### XGBoost ########\n",
    "############################\n",
    "import xgboost as xgb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "model_scoring(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 99, 256)           1280      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 99, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 49, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 48, 256)           131328    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 48, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 24, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 24, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 12, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 362,369\n",
      "Trainable params: 362,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5264 samples, validate on 2256 samples\n",
      "Epoch 1/50\n",
      "5264/5264 [==============================] - 3s 664us/sample - loss: 0.6325 - accuracy: 0.6731 - val_loss: 0.6229 - val_accuracy: 0.6862\n",
      "Epoch 2/50\n",
      "5264/5264 [==============================] - 2s 364us/sample - loss: 0.6162 - accuracy: 0.6774 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 3/50\n",
      "5264/5264 [==============================] - 3s 536us/sample - loss: 0.6022 - accuracy: 0.6829 - val_loss: 0.6117 - val_accuracy: 0.6884\n",
      "Epoch 4/50\n",
      "5264/5264 [==============================] - 2s 430us/sample - loss: 0.5882 - accuracy: 0.6979 - val_loss: 0.5880 - val_accuracy: 0.7021\n",
      "Epoch 5/50\n",
      "5264/5264 [==============================] - 2s 415us/sample - loss: 0.5762 - accuracy: 0.7046 - val_loss: 0.5787 - val_accuracy: 0.7030\n",
      "Epoch 6/50\n",
      "5264/5264 [==============================] - 2s 397us/sample - loss: 0.5563 - accuracy: 0.7164 - val_loss: 0.5658 - val_accuracy: 0.7141\n",
      "Epoch 7/50\n",
      "5264/5264 [==============================] - 3s 505us/sample - loss: 0.5370 - accuracy: 0.7356 - val_loss: 0.5493 - val_accuracy: 0.7283\n",
      "Epoch 8/50\n",
      "5264/5264 [==============================] - 2s 410us/sample - loss: 0.5173 - accuracy: 0.7492 - val_loss: 0.5349 - val_accuracy: 0.7371\n",
      "Epoch 9/50\n",
      "5264/5264 [==============================] - 2s 412us/sample - loss: 0.5025 - accuracy: 0.7587 - val_loss: 0.5284 - val_accuracy: 0.7473\n",
      "Epoch 10/50\n",
      "5264/5264 [==============================] - 3s 491us/sample - loss: 0.4965 - accuracy: 0.7633 - val_loss: 0.5318 - val_accuracy: 0.7261\n",
      "Epoch 11/50\n",
      "5264/5264 [==============================] - 2s 388us/sample - loss: 0.4697 - accuracy: 0.7813 - val_loss: 0.5037 - val_accuracy: 0.7606\n",
      "Epoch 12/50\n",
      "5264/5264 [==============================] - 2s 376us/sample - loss: 0.4490 - accuracy: 0.7865 - val_loss: 0.4772 - val_accuracy: 0.7770\n",
      "Epoch 13/50\n",
      "5264/5264 [==============================] - 2s 410us/sample - loss: 0.4309 - accuracy: 0.7990 - val_loss: 0.4696 - val_accuracy: 0.7770\n",
      "Epoch 14/50\n",
      "5264/5264 [==============================] - 2s 369us/sample - loss: 0.4192 - accuracy: 0.8003 - val_loss: 0.4757 - val_accuracy: 0.7793\n",
      "Epoch 15/50\n",
      "5264/5264 [==============================] - 2s 390us/sample - loss: 0.3918 - accuracy: 0.8241 - val_loss: 0.4497 - val_accuracy: 0.7912\n",
      "Epoch 16/50\n",
      "5264/5264 [==============================] - 2s 375us/sample - loss: 0.3760 - accuracy: 0.8317 - val_loss: 0.4481 - val_accuracy: 0.7926\n",
      "Epoch 17/50\n",
      "5264/5264 [==============================] - 2s 431us/sample - loss: 0.3569 - accuracy: 0.8376 - val_loss: 0.4266 - val_accuracy: 0.8050\n",
      "Epoch 18/50\n",
      "5264/5264 [==============================] - 2s 378us/sample - loss: 0.3493 - accuracy: 0.8461 - val_loss: 0.4165 - val_accuracy: 0.8103\n",
      "Epoch 19/50\n",
      "5264/5264 [==============================] - 2s 395us/sample - loss: 0.3278 - accuracy: 0.8608 - val_loss: 0.4320 - val_accuracy: 0.8174\n",
      "Epoch 20/50\n",
      "5264/5264 [==============================] - 2s 473us/sample - loss: 0.3177 - accuracy: 0.8659 - val_loss: 0.4150 - val_accuracy: 0.8027\n",
      "Epoch 21/50\n",
      "5264/5264 [==============================] - 3s 479us/sample - loss: 0.2980 - accuracy: 0.8701 - val_loss: 0.4020 - val_accuracy: 0.8223\n",
      "Epoch 22/50\n",
      "5264/5264 [==============================] - 2s 409us/sample - loss: 0.2865 - accuracy: 0.8792 - val_loss: 0.4038 - val_accuracy: 0.8240\n",
      "Epoch 23/50\n",
      "5264/5264 [==============================] - 2s 386us/sample - loss: 0.2725 - accuracy: 0.8854 - val_loss: 0.4047 - val_accuracy: 0.8289\n",
      "Epoch 24/50\n",
      "5264/5264 [==============================] - 2s 445us/sample - loss: 0.2578 - accuracy: 0.8885 - val_loss: 0.4036 - val_accuracy: 0.8316\n",
      "Epoch 25/50\n",
      "5264/5264 [==============================] - 2s 380us/sample - loss: 0.2519 - accuracy: 0.8965 - val_loss: 0.3875 - val_accuracy: 0.8347\n",
      "Epoch 26/50\n",
      "5264/5264 [==============================] - 2s 398us/sample - loss: 0.2234 - accuracy: 0.9050 - val_loss: 0.3802 - val_accuracy: 0.8369\n",
      "Epoch 27/50\n",
      "5264/5264 [==============================] - 2s 370us/sample - loss: 0.2249 - accuracy: 0.9025 - val_loss: 0.3867 - val_accuracy: 0.8471\n",
      "Epoch 28/50\n",
      "5264/5264 [==============================] - 2s 386us/sample - loss: 0.2144 - accuracy: 0.9111 - val_loss: 0.4104 - val_accuracy: 0.8329\n",
      "Epoch 29/50\n",
      "5264/5264 [==============================] - 2s 375us/sample - loss: 0.2092 - accuracy: 0.9139 - val_loss: 0.3808 - val_accuracy: 0.8466\n",
      "Epoch 30/50\n",
      "5264/5264 [==============================] - 2s 435us/sample - loss: 0.1943 - accuracy: 0.9240 - val_loss: 0.4129 - val_accuracy: 0.8271\n",
      "Epoch 31/50\n",
      "5264/5264 [==============================] - 2s 382us/sample - loss: 0.1862 - accuracy: 0.9248 - val_loss: 0.3908 - val_accuracy: 0.8395\n",
      "Epoch 32/50\n",
      "5264/5264 [==============================] - 3s 543us/sample - loss: 0.1866 - accuracy: 0.9246 - val_loss: 0.3965 - val_accuracy: 0.8449\n",
      "Epoch 33/50\n",
      "5264/5264 [==============================] - 2s 375us/sample - loss: 0.1700 - accuracy: 0.9337 - val_loss: 0.3944 - val_accuracy: 0.8471\n",
      "Epoch 34/50\n",
      "5264/5264 [==============================] - 2s 378us/sample - loss: 0.1638 - accuracy: 0.9339 - val_loss: 0.3815 - val_accuracy: 0.8488\n",
      "Epoch 35/50\n",
      "5264/5264 [==============================] - 3s 488us/sample - loss: 0.1584 - accuracy: 0.9379 - val_loss: 0.3860 - val_accuracy: 0.8511\n",
      "Epoch 36/50\n",
      "5264/5264 [==============================] - 2s 399us/sample - loss: 0.1571 - accuracy: 0.9411 - val_loss: 0.3950 - val_accuracy: 0.8497\n",
      "Epoch 37/50\n",
      "5264/5264 [==============================] - 2s 410us/sample - loss: 0.1495 - accuracy: 0.9413 - val_loss: 0.3896 - val_accuracy: 0.8537\n",
      "Epoch 38/50\n",
      "5264/5264 [==============================] - 2s 388us/sample - loss: 0.1389 - accuracy: 0.9474 - val_loss: 0.3826 - val_accuracy: 0.8502\n",
      "Epoch 39/50\n",
      "5264/5264 [==============================] - 2s 371us/sample - loss: 0.1328 - accuracy: 0.9510 - val_loss: 0.3660 - val_accuracy: 0.8635\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5264/5264 [==============================] - 2s 398us/sample - loss: 0.1276 - accuracy: 0.9497 - val_loss: 0.3862 - val_accuracy: 0.8613\n",
      "Epoch 41/50\n",
      "5264/5264 [==============================] - 2s 355us/sample - loss: 0.1239 - accuracy: 0.9514 - val_loss: 0.3828 - val_accuracy: 0.8617\n",
      "Epoch 42/50\n",
      "5264/5264 [==============================] - 2s 397us/sample - loss: 0.1380 - accuracy: 0.9441 - val_loss: 0.3899 - val_accuracy: 0.8511\n",
      "Epoch 43/50\n",
      "5264/5264 [==============================] - 3s 498us/sample - loss: 0.1232 - accuracy: 0.9514 - val_loss: 0.4153 - val_accuracy: 0.8497\n",
      "Epoch 44/50\n",
      "5264/5264 [==============================] - 2s 401us/sample - loss: 0.1102 - accuracy: 0.9576 - val_loss: 0.3783 - val_accuracy: 0.8648\n",
      "Epoch 45/50\n",
      "5264/5264 [==============================] - 2s 381us/sample - loss: 0.1106 - accuracy: 0.9573 - val_loss: 0.4257 - val_accuracy: 0.8595\n",
      "Epoch 46/50\n",
      "5264/5264 [==============================] - 2s 378us/sample - loss: 0.1023 - accuracy: 0.9616 - val_loss: 0.4413 - val_accuracy: 0.8568\n",
      "Epoch 47/50\n",
      "5264/5264 [==============================] - 2s 377us/sample - loss: 0.1102 - accuracy: 0.9550 - val_loss: 0.4637 - val_accuracy: 0.8480\n",
      "Epoch 48/50\n",
      "5264/5264 [==============================] - 3s 481us/sample - loss: 0.0939 - accuracy: 0.9639 - val_loss: 0.4375 - val_accuracy: 0.8604\n",
      "Epoch 49/50\n",
      "5264/5264 [==============================] - 2s 435us/sample - loss: 0.0965 - accuracy: 0.9628 - val_loss: 0.4390 - val_accuracy: 0.8582\n",
      "Epoch 50/50\n",
      "5264/5264 [==============================] - 2s 402us/sample - loss: 0.0903 - accuracy: 0.9660 - val_loss: 0.4176 - val_accuracy: 0.8635\n",
      "======================================\n",
      "======Model Performance===============\n",
      "the accuracy is:  0.8504498914055229\n",
      "the precision is： 0.7684630738522954\n",
      "the recall is:  0.7549019607843137\n",
      "the f1 score is:  0.76162215628091\n",
      "confution matrix: \n",
      " [[1971  232]\n",
      " [ 250  770]]\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    " ########## CNN ###############\n",
    "#################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D,  MaxPooling1D\n",
    "import pickle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 4, input_shape=(102,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(256, 2))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, 1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train.reshape((X_train.shape[0], X_train.shape[1], 1)), np.asarray(y_train), \n",
    "          batch_size=32, epochs=50, validation_split=0.3)\n",
    "y_pred = model.predict(X_test.reshape((X_test.shape[0],X_test.shape[1],1)))\n",
    "y_pred[y_pred>0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0\n",
    "model_scoring(y_test, y_pred.reshape((-1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/imdb_cnn_lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50)                30600     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 32,265\n",
      "Trainable params: 32,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    " ########## RNN ##########\n",
    "############################\n",
    "# https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, LSTM\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D\n",
    "######################################\n",
    "n_steps = 60\n",
    "# prepare train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "# split into samples\n",
    "X_train_steps, y_train_steps = split_sequence(X_train, y_train, n_steps)\n",
    "X_test_steps, y_test_steps = split_sequence(X_test, y_test, n_steps)\n",
    "######################################\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, 102)))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 50)                30600     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 32,265\n",
      "Trainable params: 32,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5222 samples, validate on 2239 samples\n",
      "Epoch 1/30\n",
      "5222/5222 [==============================] - 18s 3ms/sample - loss: 0.6244 - accuracy: 0.6758 - val_loss: 0.6121 - val_accuracy: 0.6771\n",
      "Epoch 2/30\n",
      "5222/5222 [==============================] - 14s 3ms/sample - loss: 0.5982 - accuracy: 0.6907 - val_loss: 0.5963 - val_accuracy: 0.6918\n",
      "Epoch 3/30\n",
      "5222/5222 [==============================] - 14s 3ms/sample - loss: 0.5718 - accuracy: 0.7051 - val_loss: 0.6011 - val_accuracy: 0.6891\n",
      "Epoch 4/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.5432 - accuracy: 0.7252 - val_loss: 0.5949 - val_accuracy: 0.6891\n",
      "Epoch 5/30\n",
      "5222/5222 [==============================] - 15s 3ms/sample - loss: 0.5160 - accuracy: 0.7476 - val_loss: 0.5846 - val_accuracy: 0.6914\n",
      "Epoch 6/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.4818 - accuracy: 0.7585 - val_loss: 0.5955 - val_accuracy: 0.7012\n",
      "Epoch 7/30\n",
      "5222/5222 [==============================] - 15s 3ms/sample - loss: 0.4437 - accuracy: 0.7903 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 8/30\n",
      "5222/5222 [==============================] - 14s 3ms/sample - loss: 0.4066 - accuracy: 0.8106 - val_loss: 0.6460 - val_accuracy: 0.6856\n",
      "Epoch 9/30\n",
      "5222/5222 [==============================] - 14s 3ms/sample - loss: 0.3643 - accuracy: 0.8368 - val_loss: 0.6771 - val_accuracy: 0.6704\n",
      "Epoch 10/30\n",
      "5222/5222 [==============================] - 12s 2ms/sample - loss: 0.3293 - accuracy: 0.8539 - val_loss: 0.7804 - val_accuracy: 0.6516\n",
      "Epoch 11/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.2789 - accuracy: 0.8815 - val_loss: 0.8726 - val_accuracy: 0.6682\n",
      "Epoch 12/30\n",
      "5222/5222 [==============================] - 14s 3ms/sample - loss: 0.2436 - accuracy: 0.8985 - val_loss: 0.8870 - val_accuracy: 0.6458\n",
      "Epoch 13/30\n",
      "5222/5222 [==============================] - 13s 2ms/sample - loss: 0.2122 - accuracy: 0.9142 - val_loss: 0.8979 - val_accuracy: 0.6641\n",
      "Epoch 14/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.1693 - accuracy: 0.9328 - val_loss: 1.0019 - val_accuracy: 0.6530\n",
      "Epoch 15/30\n",
      "5222/5222 [==============================] - 13s 2ms/sample - loss: 0.1365 - accuracy: 0.9483 - val_loss: 1.1008 - val_accuracy: 0.6472\n",
      "Epoch 16/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.1198 - accuracy: 0.9561 - val_loss: 1.1528 - val_accuracy: 0.6623\n",
      "Epoch 17/30\n",
      "5222/5222 [==============================] - 15s 3ms/sample - loss: 0.1299 - accuracy: 0.9552 - val_loss: 1.2731 - val_accuracy: 0.6494\n",
      "Epoch 18/30\n",
      "5222/5222 [==============================] - 13s 2ms/sample - loss: 0.0927 - accuracy: 0.9699 - val_loss: 1.4005 - val_accuracy: 0.6588\n",
      "Epoch 19/30\n",
      "5222/5222 [==============================] - 13s 2ms/sample - loss: 0.0769 - accuracy: 0.9757 - val_loss: 1.5756 - val_accuracy: 0.6695\n",
      "Epoch 20/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.0502 - accuracy: 0.9866 - val_loss: 1.6352 - val_accuracy: 0.6597\n",
      "Epoch 21/30\n",
      "5222/5222 [==============================] - 12s 2ms/sample - loss: 0.0417 - accuracy: 0.9885 - val_loss: 1.7085 - val_accuracy: 0.6440\n",
      "Epoch 22/30\n",
      "5222/5222 [==============================] - 15s 3ms/sample - loss: 0.0459 - accuracy: 0.9864 - val_loss: 1.8398 - val_accuracy: 0.6490\n",
      "Epoch 23/30\n",
      "5222/5222 [==============================] - 12s 2ms/sample - loss: 0.0673 - accuracy: 0.9784 - val_loss: 1.7577 - val_accuracy: 0.6659\n",
      "Epoch 24/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.0474 - accuracy: 0.9851 - val_loss: 1.8747 - val_accuracy: 0.6699\n",
      "Epoch 25/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.0803 - accuracy: 0.9743 - val_loss: 1.8426 - val_accuracy: 0.6543\n",
      "Epoch 26/30\n",
      "5222/5222 [==============================] - 13s 3ms/sample - loss: 0.0350 - accuracy: 0.9906 - val_loss: 1.8765 - val_accuracy: 0.6445\n",
      "Epoch 27/30\n",
      "5222/5222 [==============================] - 14s 3ms/sample - loss: 0.0344 - accuracy: 0.9912 - val_loss: 2.0529 - val_accuracy: 0.6565\n",
      "Epoch 28/30\n",
      "5222/5222 [==============================] - 12s 2ms/sample - loss: 0.0169 - accuracy: 0.9975 - val_loss: 2.2110 - val_accuracy: 0.6615\n",
      "Epoch 29/30\n",
      "5222/5222 [==============================] - 14s 3ms/sample - loss: 0.0079 - accuracy: 0.9992 - val_loss: 2.2604 - val_accuracy: 0.6556\n",
      "Epoch 30/30\n",
      "5222/5222 [==============================] - 13s 2ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.3588 - val_accuracy: 0.6521\n",
      "======================================\n",
      "======Model Performance===============\n",
      "the accuracy is:  0.6823640960809102\n",
      "the precision is： 0.5\n",
      "the recall is:  0.43482587064676614\n",
      "the f1 score is:  0.4651410324640766\n",
      "confution matrix: \n",
      " [[1722  437]\n",
      " [ 568  437]]\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "model.fit(X_train_steps, y_train_steps, \n",
    "          batch_size=32, epochs=30, validation_split=0.3)\n",
    "######################################\n",
    "y_pred = model.predict(X_test_steps)\n",
    "y_pred[y_pred>0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0\n",
    "model_scoring(y_test_steps, y_pred)\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
